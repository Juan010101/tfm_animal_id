{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cac62e4-cc32-4c84-8a1d-fd68506731d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juanc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from wildlife_datasets import datasets, splits\n",
    "import torchvision.transforms as T\n",
    "import wildlife_tools.data.__init__ as tools\n",
    "import timm\n",
    "from wildlife_tools.features import DeepFeatures\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from active_semi_clustering.semi_supervised.pairwise_constraints import PCKMeans, COPKMeans, MKMeans, MPCKMeans, MPCKMeans, RCAKMeans\n",
    "from active_semi_clustering.semi_supervised.labeled_data import KMeans\n",
    "from active_semi_clustering.active.pairwise_constraints import ExampleOracle, ExploreConsolidate, MinMax\n",
    "from sklearn import metrics\n",
    "from active_semi_clustering.active.pairwise_constraints import ExampleOracle, ExploreConsolidate, MinMax\n",
    "import numpy as np\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb959df-e7f6-4990-a21d-f059249efb6e",
   "metadata": {},
   "source": [
    "## Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d512174a-0b9d-48c8-a566-353e63ca136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET SeaTurtleID2022: DOWNLOADING STARTED.\n",
      "DATASET SeaTurtleID2022: EXTRACTING STARTED.\n",
      "DATASET SeaTurtleID2022: FINISHED.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el dataset de tortugas\n",
    "dataset_path = 'data/SeaTurtleID2022'\n",
    "datasets.SeaTurtleID2022.get_data(dataset_path)\n",
    "metadata = datasets.SeaTurtleID2022(dataset_path)\n",
    "data_df = metadata.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d4d36f-9885-482c-b558-94c18a416d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET CowDataset: DOWNLOADING STARTED.\n",
      "You are trying to download an already downloaded dataset.\n",
      "        This message may have happened to due interrupted download or extract.\n",
      "        To force the download use the `force=True` keyword such as\n",
      "        get_data(..., force=True) or download(..., force=True).\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Cargamos el dataset de tortugas\n",
    "dataset_path = 'data/CowDataset'\n",
    "datasets.CowDataset.get_data(dataset_path)\n",
    "metadata = datasets.CowDataset(dataset_path)\n",
    "data_df = metadata.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0abc0-2e3a-4c26-a2e3-a33b5dbe2474",
   "metadata": {},
   "source": [
    "## Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "130d47a1-99cf-4659-a2c8-bf770bc95244",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.Resize(size=(384, 384)),\n",
    "                              T.ToTensor(), \n",
    "                              T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "dataset = tools.WildlifeDataset(metadata=data_df, root=\"./data/SeaTurtleID2022\", transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cc078a-e8b0-4375-8749-e123b6818892",
   "metadata": {},
   "source": [
    "## Extracción de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e962c327-fa2a-4df3-84a2-e0c58a42f4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                            | 1/69 [04:41<5:18:33, 281.08s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 5176) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\multiprocessing\\reductions.py:560\u001b[0m, in \u001b[0;36mrebuild_storage_filename\u001b[1;34m(cls, manager, handle, size, dtype)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_shared_filename_cpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Couldn't open shared event: <torch_5176_892145977_2_event>, error code: <2>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m backboneDescriptor \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mcreate_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf-hub:BVRA/MegaDescriptor-L-384\u001b[39m\u001b[38;5;124m\"\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      2\u001b[0m extractorDescritor \u001b[38;5;241m=\u001b[39m DeepFeatures(backboneDescriptor)\n\u001b[1;32m----> 3\u001b[0m outputFeaturesDescritor \u001b[38;5;241m=\u001b[39m \u001b[43mextractorDescritor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wildlife_tools\\features\\deep.py:48\u001b[0m, in \u001b[0;36mDeepFeatures.__call__\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     41\u001b[0m loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m     42\u001b[0m     dataset,\n\u001b[0;32m     43\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers,\n\u001b[0;32m     44\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m     45\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 48\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmininterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1255\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1258\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 5176) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "backboneDescriptor = timm.create_model(\"hf-hub:BVRA/MegaDescriptor-L-384\", pretrained=True, num_classes=0)\n",
    "extractorDescritor = DeepFeatures(backboneDescriptor)\n",
    "outputFeaturesDescritor = extractorDescritor(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "214a86e8-74c7-4088-befa-e951ad164386",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputClasses = data_df['identity'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f7d96-0fe8-4c0a-963f-e22ad80474d9",
   "metadata": {},
   "source": [
    "## Extracción del numero de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fd393ee-5381-4aa4-88bb-aa01dfa45dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfCluster = data_df['identity'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63275a0-20ea-47ac-8229-66c7e826590d",
   "metadata": {},
   "source": [
    "## Refinamiento de Caracteristicas Extraidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e3955cd-9638-4525-807c-a0fa24094c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_visualized = tsne.fit_transform(outputFeaturesDescritor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "351ec258-f4c5-40fb-a25b-eafb23fb385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_visualized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5e7e8-e274-4c7e-88b5-b526c5f6cfca",
   "metadata": {},
   "source": [
    "## Extracción de restricciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19bc50a1-5cfb-473e-8eda-6ab996607749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold must-link basado en densidad: 0.0051595037686212675\n"
     ]
    }
   ],
   "source": [
    "def density_based_threshold_must(X, percentile=20):\n",
    "    # Entrena un modelo de vecinos más cercanos\n",
    "    nbrs = NearestNeighbors(n_neighbors=numberOfCluster).fit(X)\n",
    "    distances, _ = nbrs.kneighbors(X)\n",
    "    \n",
    "    # Selecciona las distancias al vecino más cercano (primer vecino, excepto el punto en sí)\n",
    "    avg_distances = distances[:, 1]  # Distancia al vecino más cercano real\n",
    "    return np.percentile(avg_distances, percentile)  # Percentil bajo para zonas densas\n",
    "\n",
    "# Aplica la función al dataset\n",
    "threshold_must = density_based_threshold_must(X_scaled)\n",
    "print(\"Threshold must-link basado en densidad:\", threshold_must)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36853c55-62fd-4983-899d-59f69b6356a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold cannot-link basado en densidad: 0.17314173210384964\n"
     ]
    }
   ],
   "source": [
    "def density_based_threshold(X, percentile=80):\n",
    "    nbrs = NearestNeighbors(n_neighbors=13).fit(X)\n",
    "    distances, _ = nbrs.kneighbors(X)\n",
    "    avg_distances = distances[:, -1]  # Distancia al vecino más lejano entre los 10 más cercanos\n",
    "    return np.percentile(avg_distances, percentile)\n",
    "\n",
    "threshold_cannot = density_based_threshold(X_scaled)\n",
    "print(\"Threshold cannot-link basado en densidad:\", threshold_cannot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62f18ec1-6509-41d7-9018-458b92679489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricciones must-link: [(6, 399), (10, 1102), (18, 22), (19, 869), (23, 1389)]\n",
      "Restricciones cannot-link: [(0, 1), (0, 3), (0, 4), (0, 5), (0, 6)]\n"
     ]
    }
   ],
   "source": [
    "def generate_constraints(X, y, threshold_must, threshold_cannot):\n",
    "    distances = euclidean_distances(X)\n",
    "    must_link = []\n",
    "    cannot_link = []\n",
    "    \n",
    "    # Recorre combinaciones de pares de puntos\n",
    "    for i, j in combinations(range(len(y)), 2):\n",
    "        if distances[i, j] <= threshold_must and y[i] == y[j]:  # misma etiqueta, distancia corta\n",
    "            must_link.append((i, j))\n",
    "        elif distances[i, j] >= threshold_cannot and y[i] != y[j]:  # diferente etiqueta, distancia larga\n",
    "            cannot_link.append((i, j))\n",
    "    \n",
    "    return must_link, cannot_link \n",
    "\n",
    "must_link, cannot_link = generate_constraints(X_scaled, outputClasses, threshold_must, threshold_cannot)\n",
    "\n",
    "print(\"Restricciones must-link:\", must_link[:5])  # Muestra las primeras 5 restricciones must-link\n",
    "print(\"Restricciones cannot-link:\", cannot_link[:5])  # Muestra las primeras 5 restricciones cannot-link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24cb92ba-2abd-48ca-ba60-be240581688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement your own oracle that will, for example, query a domain expert via GUI or CLI\n",
    "oracle = ExampleOracle(outputClasses, max_queries_cnt=20)\n",
    "\n",
    "active_learner = MinMax(n_clusters=numberOfCluster)\n",
    "active_learner.fit(outputFeaturesDescritor, oracle=oracle)\n",
    "pairwise_constraints = active_learner.pairwise_constraints_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1126352-84e6-459f-b883-0640e73057bb",
   "metadata": {},
   "source": [
    "## Evaluaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2ccec-7dcd-4856-9f1a-1e9c301f530d",
   "metadata": {},
   "source": [
    "## Supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11d7ba79-d034-4e92-9749-803f73b33d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9932659932659933\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        20\n",
      "           2       1.00      1.00      1.00        21\n",
      "           3       1.00      1.00      1.00        15\n",
      "           4       0.96      1.00      0.98        26\n",
      "           5       1.00      1.00      1.00        27\n",
      "           6       0.96      1.00      0.98        26\n",
      "           7       1.00      0.95      0.98        22\n",
      "           8       1.00      0.96      0.98        25\n",
      "           9       1.00      1.00      1.00        24\n",
      "          10       1.00      1.00      1.00        28\n",
      "          11       1.00      1.00      1.00        18\n",
      "          12       1.00      1.00      1.00        26\n",
      "          13       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.99       297\n",
      "   macro avg       0.99      0.99      0.99       297\n",
      "weighted avg       0.99      0.99      0.99       297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(outputFeaturesDescritor, outputClasses, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Escalar los datos\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. Entrenar un clasificador (por ejemplo, SVM)\n",
    "clf = SVC(kernel='linear', random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. Predecir en los datos de prueba\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# 5. Evaluar el rendimiento\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ee677-ec46-4458-911c-f9294617fe2d",
   "metadata": {},
   "source": [
    "## Semi Supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b7d16d2-ba06-4bc0-a5fa-64fae5bfa424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand Score Promedio: 0.6473 ± 0.0362\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10  # Número de repeticiones\n",
    "rand_scores = []\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    clusterer = PCKMeans(n_clusters=numberOfCluster, max_iter=1000, w=0)\n",
    "    clusterer.fit(X_scaled, ml=pairwise_constraints[0], cl=pairwise_constraints[1])\n",
    "    score = adjusted_rand_score(outputClasses, clusterer.labels_)\n",
    "    rand_scores.append(score)\n",
    "\n",
    "mean_rand_score = np.mean(rand_scores)\n",
    "std_rand_score = np.std(rand_scores)\n",
    "\n",
    "print(f\"Rand Score Promedio: {mean_rand_score:.4f} ± {std_rand_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "321159bb-9bff-4e4f-95e0-1ab1f3e19d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand Score Promedio: 0.9874 ± 0.0084\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10  # Número de repeticiones\n",
    "rand_scores = []\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    clusterer = PCKMeans(n_clusters=numberOfCluster, max_iter=100, w=1)\n",
    "    clusterer.fit(X_scaled, outputClasses, ml=must_link, cl=cannot_link)\n",
    "    score = adjusted_rand_score(outputClasses, clusterer.labels_)\n",
    "    rand_scores.append(score)\n",
    "\n",
    "mean_rand_score = np.mean(rand_scores)\n",
    "std_rand_score = np.std(rand_scores)\n",
    "\n",
    "print(f\"Rand Score Promedio: {mean_rand_score:.4f} ± {std_rand_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff527e3f-d3b7-4b20-9ea3-5d5c267cc112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneidad: 1.0000\n",
      "Completitud: 1.0000\n",
      "V-Measure: 1.0000\n"
     ]
    }
   ],
   "source": [
    "homogeneity = homogeneity_score(outputClasses, clusterer.labels_)\n",
    "completeness = completeness_score(outputClasses, clusterer.labels_)\n",
    "v_measure = v_measure_score(outputClasses, clusterer.labels_)\n",
    "\n",
    "print(f\"Homogeneidad: {homogeneity:.4f}\")\n",
    "print(f\"Completitud: {completeness:.4f}\")\n",
    "print(f\"V-Measure: {v_measure:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123e0fe-8591-4726-b984-4fd9f12a93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10  # Número de repeticiones\n",
    "rand_scores = []\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    clusterer = PCKMeans(n_clusters=numberOfCluster, max_iter=100, w=1)\n",
    "    clusterer.fit(X_scaled, ml=must_link, cl=cannot_link)\n",
    "    score = adjusted_rand_score(outputClasses, clusterer.labels_)\n",
    "    rand_scores.append(score)\n",
    "\n",
    "mean_rand_score = np.mean(rand_scores)\n",
    "std_rand_score = np.std(rand_scores)\n",
    "\n",
    "print(f\"Rand Score Promedio: {mean_rand_score:.4f} ± {std_rand_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c2adb55-56a4-4774-a063-26aabcaf2cd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyClustersException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyClustersException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_runs):\n\u001b[0;32m      5\u001b[0m     clusterer \u001b[38;5;241m=\u001b[39m COPKMeans(n_clusters\u001b[38;5;241m=\u001b[39mnumberOfCluster, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mclusterer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmust_link\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcannot_link\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     score \u001b[38;5;241m=\u001b[39m adjusted_rand_score(outputClasses, clusterer\u001b[38;5;241m.\u001b[39mlabels_)\n\u001b[0;32m      8\u001b[0m     rand_scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\active_semi_supervised_clustering-0.0.1-py3.12.egg\\active_semi_clustering\\semi_supervised\\pairwise_constraints\\copkmeans.py:23\u001b[0m, in \u001b[0;36mCOPKMeans.fit\u001b[1;34m(self, X, y, ml, cl)\u001b[0m\n\u001b[0;32m     20\u001b[0m prev_cluster_centers \u001b[38;5;241m=\u001b[39m cluster_centers\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Assign clusters\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster_centers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mml_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcl_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Estimate means\u001b[39;00m\n\u001b[0;32m     26\u001b[0m cluster_centers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_cluster_centers(X, labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\active_semi_supervised_clustering-0.0.1-py3.12.egg\\active_semi_clustering\\semi_supervised\\pairwise_constraints\\copkmeans.py:49\u001b[0m, in \u001b[0;36mCOPKMeans._assign_clusters\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m retries_cnt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries_cnt):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_assign_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ClusteringNotFoundException:\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\active_semi_supervised_clustering-0.0.1-py3.12.egg\\active_semi_clustering\\semi_supervised\\pairwise_constraints\\copkmeans.py:80\u001b[0m, in \u001b[0;36mCOPKMeans._try_assign_clusters\u001b[1;34m(self, X, cluster_centers, dist, ml_graph, cl_graph)\u001b[0m\n\u001b[0;32m     77\u001b[0m empty_clusters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(n_samples_in_cluster \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(empty_clusters) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EmptyClustersException\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m labels\n",
      "\u001b[1;31mEmptyClustersException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_runs = 10  # Número de repeticiones\n",
    "rand_scores = []\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    clusterer = COPKMeans(n_clusters=numberOfCluster, max_iter=100)\n",
    "    clusterer.fit(X_scaled, outputClasses, ml=must_link, cl=cannot_link)\n",
    "    score = adjusted_rand_score(outputClasses, clusterer.labels_)\n",
    "    rand_scores.append(score)\n",
    "\n",
    "mean_rand_score = np.mean(rand_scores)\n",
    "std_rand_score = np.std(rand_scores)\n",
    "\n",
    "\n",
    "print(f\"Rand Score Promedio: {mean_rand_score:.4f} ± {std_rand_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e2815-e0a4-4fca-bcaa-08a657aba876",
   "metadata": {},
   "outputs": [],
   "source": [
    "homogeneity = homogeneity_score(outputClasses, clusterer.labels_)\n",
    "completeness = completeness_score(outputClasses, clusterer.labels_)\n",
    "v_measure = v_measure_score(outputClasses, clusterer.labels_)\n",
    "\n",
    "print(f\"Homogeneidad: {homogeneity:.4f}\")\n",
    "print(f\"Completitud: {completeness:.4f}\")\n",
    "print(f\"V-Measure: {v_measure:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8469d-4d9e-46aa-8faa-b2e6a6eaf440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da630bb-21a1-4760-8e05-e14a2a09d963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062d407-dd18-42b2-a4b1-8cac5e46783b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
