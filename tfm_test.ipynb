{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juan010101/tfm_animal_id/blob/main/tfm_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Juan010101/TFM_Project.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSuv_Wek3rRw",
        "outputId": "e721361a-29b5-4335-a652-8b181b285b1b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TFM_Project'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAuKUFnsoXn_",
        "outputId": "fbad699f-71af-4f91-ecad-86a9ab50e74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wildlife_tools\n",
            "  Downloading wildlife_tools-0.0.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from wildlife_tools) (1.0.11)\n",
            "Requirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.10/dist-packages (from wildlife_tools) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from wildlife_tools) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.10/dist-packages (from wildlife_tools) (4.66.6)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.62 in /usr/local/lib/python3.10/dist-packages (from wildlife_tools) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from wildlife_tools) (11.0.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from wildlife_tools) (1.5.2)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from wildlife_tools) (2.0.8)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from wildlife_tools) (2.17.0)\n",
            "Collecting pytorch-metric-learning (from wildlife_tools)\n",
            "  Downloading pytorch_metric_learning-2.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: transformers>=4.30.2 in /usr/local/lib/python3.10/dist-packages (from wildlife_tools) (4.46.2)\n",
            "Collecting wildlife-datasets>=0.3.4 (from wildlife_tools)\n",
            "  Downloading wildlife_datasets-1.0.5-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting kornia>=0.6.12 (from wildlife_tools)\n",
            "  Downloading kornia-0.7.4-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting faiss-gpu (from wildlife_tools)\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting kornia-rs>=0.1.0 (from kornia>=0.6.12->wildlife_tools)\n",
            "  Downloading kornia_rs-0.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia>=0.6.12->wildlife_tools) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->wildlife_tools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->wildlife_tools) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->wildlife_tools) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->wildlife_tools) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->wildlife_tools) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->wildlife_tools) (3.5.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.2->wildlife_tools) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.2->wildlife_tools) (0.26.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.2->wildlife_tools) (0.4.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.2->wildlife_tools) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.2->wildlife_tools) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.2->wildlife_tools) (0.20.3)\n",
            "Requirement already satisfied: matplotlib>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from wildlife-datasets>=0.3.4->wildlife_tools) (3.8.0)\n",
            "Collecting datasets (from wildlife-datasets>=0.3.4->wildlife_tools)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from wildlife-datasets>=0.3.4->wildlife_tools) (5.2.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from wildlife-datasets>=0.3.4->wildlife_tools) (1.6.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->wildlife_tools) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->wildlife_tools) (1.67.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->wildlife_tools) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->wildlife_tools) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->wildlife_tools) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->wildlife_tools) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->wildlife_tools) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->wildlife_tools) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->wildlife-datasets>=0.3.4->wildlife_tools) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->wildlife-datasets>=0.3.4->wildlife_tools) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->wildlife-datasets>=0.3.4->wildlife_tools) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->wildlife-datasets>=0.3.4->wildlife_tools) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->wildlife-datasets>=0.3.4->wildlife_tools) (3.2.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->wildlife-datasets>=0.3.4->wildlife_tools) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->wildlife-datasets>=0.3.4->wildlife_tools)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets->wildlife-datasets>=0.3.4->wildlife_tools)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->wildlife-datasets>=0.3.4->wildlife_tools)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->wildlife-datasets>=0.3.4->wildlife_tools) (3.10.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.2->wildlife_tools) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.2->wildlife_tools) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.2->wildlife_tools) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.2->wildlife_tools) (2024.8.30)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->wildlife-datasets>=0.3.4->wildlife_tools) (4.12.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->wildlife-datasets>=0.3.4->wildlife_tools) (8.0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->wildlife-datasets>=0.3.4->wildlife_tools) (6.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->wildlife-datasets>=0.3.4->wildlife_tools) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->wildlife-datasets>=0.3.4->wildlife_tools) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->wildlife-datasets>=0.3.4->wildlife_tools) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->wildlife-datasets>=0.3.4->wildlife_tools) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->wildlife-datasets>=0.3.4->wildlife_tools) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->wildlife-datasets>=0.3.4->wildlife_tools) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->wildlife-datasets>=0.3.4->wildlife_tools) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->wildlife-datasets>=0.3.4->wildlife_tools) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->wildlife-datasets>=0.3.4->wildlife_tools) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->wildlife-datasets>=0.3.4->wildlife_tools) (1.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->wildlife-datasets>=0.3.4->wildlife_tools) (1.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->wildlife-datasets>=0.3.4->wildlife_tools) (0.2.0)\n",
            "Downloading wildlife_tools-0.0.9-py3-none-any.whl (25 kB)\n",
            "Downloading kornia-0.7.4-py2.py3-none-any.whl (899 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.4/899.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wildlife_datasets-1.0.5-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_metric_learning-2.7.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu, xxhash, kornia-rs, fsspec, dill, multiprocess, pytorch-metric-learning, kornia, datasets, wildlife-datasets, wildlife_tools\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 faiss-gpu-1.7.2 fsspec-2024.9.0 kornia-0.7.4 kornia-rs-0.1.7 multiprocess-0.70.16 pytorch-metric-learning-2.7.0 wildlife-datasets-1.0.5 wildlife_tools-0.0.9 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wildlife_tools torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSze2yOGoiTZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "from wildlife_datasets.datasets import SeaTurtleID2022\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVOxaR4sowBw",
        "outputId": "a4ac7dc2-6128-420c-81ca-379b19862685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATASET SeaTurtleID2022: DOWNLOADING STARTED.\n",
            "DATASET SeaTurtleID2022: EXTRACTING STARTED.\n",
            "DATASET SeaTurtleID2022: FINISHED.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Descargar el dataset SeaTurtleID2022\n",
        "metadata = SeaTurtleID2022.get_data('data/SeaTurtleID2022')\n",
        "dataset = SeaTurtleID2022('data/SeaTurtleID2022')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvEmlD75o53X"
      },
      "outputs": [],
      "source": [
        "# Convertir a DataFrame y hacer preprocesamiento básico\n",
        "df = dataset.df\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL0lSEq9pW34"
      },
      "outputs": [],
      "source": [
        "# Filtramos las columnas necesarias y hacemos un split para entrenamiento y test\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eOwg6hVwSLO"
      },
      "outputs": [],
      "source": [
        "# Mapear las etiquetas a índices numéricos para clasificación\n",
        "label_mapping = {label: idx for idx, label in enumerate(train_df['identity'].unique())}\n",
        "train_df['label_idx'] = train_df['identity'].map(label_mapping)\n",
        "test_df['label_idx'] = test_df['identity'].map(label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9vznI5ZphWf"
      },
      "outputs": [],
      "source": [
        "# Definir transformaciones para imágenes\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs5_80kjpjQu"
      },
      "outputs": [],
      "source": [
        "# Clase Custom Dataset\n",
        "class SeaTurtleDataset(Dataset):\n",
        "    def __init__(self, dataframe, root_dir, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.dataframe.iloc[idx, 2])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.dataframe.iloc[idx]['label_idx']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.tensor(label, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bOUgQ5vpltn"
      },
      "outputs": [],
      "source": [
        "# Crear instancias de los datasets de entrenamiento y test\n",
        "train_dataset = SeaTurtleDataset(train_df, 'data/SeaTurtleID2022', transform=transform)\n",
        "test_dataset = SeaTurtleDataset(test_df, 'data/SeaTurtleID2022', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClxDVoImponG"
      },
      "outputs": [],
      "source": [
        "# Crear DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1neXSIEpqlO"
      },
      "outputs": [],
      "source": [
        "# Definir un modelo CNN sencillo\n",
        "class TurtleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(TurtleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 28 * 28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4b9NhhUptcm"
      },
      "outputs": [],
      "source": [
        "# Instanciar el modelo\n",
        "num_classes = len(label_mapping)\n",
        "model = TurtleCNN(num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFRV1Pnlpvlm",
        "outputId": "93c2d7f2-dc4d-4799-c511-62a0101f627e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TurtleCNN(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=100352, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=438, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a14D__9Cp0Hu"
      },
      "outputs": [],
      "source": [
        "# Definir función de pérdida y optimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scheduler para ajustar la tasa de aprendizaje\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
      ],
      "metadata": {
        "id": "szb20aiLfJde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjz9zOZ8p28W",
        "outputId": "963d0170-fbb6-4426-b3bf-664ccd2f43a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 5.7121\n",
            "Epoch [2/5], Loss: 5.3836\n",
            "Epoch [3/5], Loss: 5.2075\n",
            "Epoch [4/5], Loss: 5.0566\n",
            "Epoch [5/5], Loss: 4.9395\n"
          ]
        }
      ],
      "source": [
        "# Entrenar el modelo\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward + backward + optimización\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww5Rh1Fdp6We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "638e4b27-38e6-4143-971f-f792b10d74f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 6.01%\n"
          ]
        }
      ],
      "source": [
        "# Evaluar el modelo en el dataset de test\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PNvSSk2KcXyY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5Vm9uu0wLqQ802cEwkwEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}